{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import html\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from unstructured.partition.html import partition_html\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from email.utils import parsedate_to_datetime\n",
    "from openai import OpenAI\n",
    "from exa_py.api import Exa  # Import Exa from exa_py.api\n",
    "from PIL import Image\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "import yfinance as yf\n",
    "from textblob import TextBlob\n",
    "from forex_python.converter import CurrencyRates\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply asyncio patch to handle nested loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBlogGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        feed_file=\"rss_feeds.json\",\n",
    "        portfolio_file=\"portfolio.json\",\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        tavily_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "    ):\n",
    "        self.feed_file = feed_file\n",
    "        self.portfolio_file = portfolio_file\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "        ]\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        self.exa_client = Exa(api_key=exa_api_key)\n",
    "        self.days_back = 7  # Analyze news from the past 7 days\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.currency_rates = CurrencyRates()\n",
    "        self.cg = CoinGeckoAPI()\n",
    "        self.tavily_api_key = \"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\"\n",
    "        self.tavily_client = TavilyClient(self.tavily_api_key)\n",
    "\n",
    "    def read_feed_urls(self):\n",
    "        \"\"\"Read RSS feed URLs from a JSON file.\"\"\"\n",
    "        with open(self.feed_file, \"r\") as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def read_portfolio(self):\n",
    "        \"\"\"Read portfolio data from a JSON file.\"\"\"\n",
    "        with open(self.portfolio_file, \"r\") as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def clean_html(self, content):\n",
    "        \"\"\"Remove HTML tags and clean up text.\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(\n",
    "                content,\n",
    "                \"html.parser\",\n",
    "                headers={\"User-Agent\": random.choice(self.user_agents)},\n",
    "            )\n",
    "            clean_text = \" \".join(soup.stripped_strings)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error cleaning HTML: {str(e)}\")\n",
    "            clean_text = content\n",
    "        return html.unescape(clean_text)\n",
    "\n",
    "    async def scrape_article_content(self, url):\n",
    "        \"\"\"Scrape article content from a URL.\"\"\"\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(self.user_agents),\n",
    "            \"Accept\": \"text/html\",\n",
    "        }\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(url, headers=headers) as response:\n",
    "                    response.raise_for_status()\n",
    "                    elements = partition_html(text=await response.text())\n",
    "                    content = \" \".join([str(element) for element in elements])\n",
    "                    clean_content = self.clean_html(content)\n",
    "                    logging.info(f\"Scraped content for {url}: {clean_content[:500]}...\")\n",
    "                    return clean_content\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return \"Unable to scrape content\"\n",
    "\n",
    "    async def generate_blog_post(\n",
    "        self, article_content, ticker, ticker_data=None, sentiment=None\n",
    "    ):\n",
    "        \"\"\"Generate a blog post using AI.\"\"\"\n",
    "\n",
    "        if ticker_data and sentiment:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, ticker data, and sentiment analysis, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data and sentiment analysis, where appropriate.\n",
    "            Finally, suggest a market action (buy, sell, or hold) for {ticker} based on the analysis.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            Sentiment Analysis: {sentiment}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        elif ticker_data:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content and ticker data, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data, where appropriate.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, write a new blog post about {ticker}. The new post should be \n",
    "            original, engaging, and informative, while capturing the key points and insights from \n",
    "            the source material.\n",
    "            Source content: {article_content[:4000]} \n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"llama-3.2-3b-preview\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a professional blogger who writes engaging and informative content across various topics, including technology, lifestyle, health, and culture. Adapt your tone and style to suit each subject while ensuring your writing is relatable and thought-provoking. When suggesting market actions, be cautious and avoid making strong recommendations. Use phrases like 'might be a good time to consider', 'could be an opportunity to', or 'investors may want to watch' to express potential actions without giving direct financial advice.\",\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "                if response.choices and response.choices[0].message.content:\n",
    "                    blog_post_content = response.choices[0].message.content.strip()\n",
    "\n",
    "                    # Assuming the new title is returned at the beginning of the content\n",
    "                    title = (\n",
    "                        blog_post_content.split(\"\\n\")[0] if blog_post_content else \"\"\n",
    "                    )  # Handle potential None\n",
    "                    new_blog_post = (\n",
    "                        \"\\n\".join(blog_post_content.split(\"\\n\")[1:])\n",
    "                        if blog_post_content\n",
    "                        else \"\"\n",
    "                    )  # Handle potential None\n",
    "\n",
    "                    return (\n",
    "                        title,\n",
    "                        new_blog_post,\n",
    "                    )  # Return both title and content\n",
    "                else:\n",
    "                    logging.error(\"OpenAI response is empty or incomplete.\")\n",
    "                    return None, None\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    wait_time = 2**attempt  # Exponential backoff\n",
    "                    logging.warning(\n",
    "                        f\"Rate limit exceeded. Waiting for {wait_time} seconds.\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logging.error(f\"Error generating blog post: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "    def find_similar_content(self, url, max_results=5):\n",
    "        \"\"\"Find similar articles using the Exa API.\"\"\"\n",
    "        try:\n",
    "            response = self.exa_client.find_similar_and_contents(\n",
    "                url, text=True, num_results=max_results\n",
    "            )\n",
    "            similar_articles = []\n",
    "            for result in response.results:\n",
    "                similar_articles.append(\n",
    "                    {\n",
    "                        \"title\": result.title,\n",
    "                        \"url\": result.url,\n",
    "                        \"snippet\": (\n",
    "                            result.text[250:] if hasattr(result, \"text\") else None\n",
    "                        ),  # snippet attribute may not exist\n",
    "                        \"text\": result.text if hasattr(result, \"text\") else None,\n",
    "                    }\n",
    "                )\n",
    "            return similar_articles\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error finding similar content for URL '{url}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt,\n",
    "        output_dir=\".\",\n",
    "        width=1024,\n",
    "        height=768,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=9,\n",
    "        scheduler=\"heunpp2\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate an image using the Hugging Face FLUX.1-dev model.\"\"\"\n",
    "        API_URL = (\n",
    "            \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "        )\n",
    "        API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "        headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"scheduler\": scheduler,\n",
    "                **kwargs,  # Pass additional parameters here\n",
    "            },\n",
    "        }\n",
    "\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(API_URL, headers=headers, json=payload)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    try:\n",
    "                        image = Image.open(io.BytesIO(response.content))\n",
    "                        timestamp = int(time.time())\n",
    "                        filename = f\"image_{timestamp}.png\"\n",
    "                        output_path = os.path.join(output_dir, filename)\n",
    "                        image.save(output_path)\n",
    "                        return image, output_path\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing image data: {str(e)}\")\n",
    "                        return None, None\n",
    "                else:\n",
    "                    logging.error(f\"Image generation API failed: {response.text}\")\n",
    "                    return None, None\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if \"Model too busy\" in str(e):\n",
    "                    wait_time = 2**attempt  # Exponential backoff\n",
    "                    logging.warning(\n",
    "                        f\"Model too busy. Waiting for {wait_time} seconds before retrying.\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logging.error(f\"Error generating image: {str(e)}\")\n",
    "                    return None, None\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating image: {str(e)}\")\n",
    "                return None, None\n",
    "\n",
    "    logging.error(\"Failed to generate image after multiple retries.\")\n",
    "    return None, None\n",
    "\n",
    "    def extract_nouns(self, text):\n",
    "        \"\"\"Extract unique nouns from text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        return list(set(nouns))  # Return unique nouns\n",
    "\n",
    "    def generate_refined_prompt_for_blog_post(self, post):\n",
    "        \"\"\"Generate a refined prompt for AI image generation.\"\"\"\n",
    "        summary = self.summarize_with_spacy(post[\"content\"])\n",
    "        nouns = self.extract_nouns(summary)\n",
    "        key_elements = \", \".join(nouns[:7])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Create a visually-rich prompt for AI image generation based on the summary of this blog post:\n",
    "\n",
    "        {summary}\n",
    "\n",
    "        Key visual themes: {key_elements}\n",
    "\n",
    "        The prompt should:\n",
    "        1. Be concise and focused on visual elements.\n",
    "        2. Seamlessly incorporate key visual concepts without explicitly listing them.\n",
    "        3. Maintain a professional tone inline with {summary} .\n",
    "        4. Be optimized for AI image generation models, emphasizing clarity and detail.\n",
    "\n",
    "        Please provide only the refined prompt without any additional commentary or explanation. The example should serve as inspiration without replicating its structure or content directly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at creating prompts for AI image generation.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=150,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            return (\n",
    "                response.choices[0].message.content.strip()\n",
    "                if response.choices[0].message.content\n",
    "                else \"A modern illustration representing key themes from the blog post.\"\n",
    "            )  # Handle potential None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating refined prompt: {str(e)}\")\n",
    "            return \"A modern illustration representing key themes from the blog post.\"\n",
    "\n",
    "    def summarize_with_spacy(self, text):\n",
    "        \"\"\"Summarize text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]\n",
    "        return \" \".join(sentences[:7])\n",
    "\n",
    "    def extract_tickers(self, text):\n",
    "        \"\"\"Extract potential stock tickers from text using regular expressions.\"\"\"\n",
    "        ticker_pattern = r\"\\b([A-Z]{2,}[A-Z0-9\\.\\-]*|[A-Z]{3,}/[A-Z]{3,})\\b\"\n",
    "        tickers = re.findall(ticker_pattern, text)\n",
    "        return list(set(tickers))  # Return unique tickers\n",
    "\n",
    "    def fetch_ticker_data(self, ticker):\n",
    "        \"\"\"Fetch relevant ticker data using yfinance and CoinGeckoAPI.\"\"\"\n",
    "        ticker_data = {}\n",
    "        try:\n",
    "            # Check if it's a forex pair (e.g., EUR/USD)\n",
    "            if \"/\" in ticker:\n",
    "                base_currency, quote_currency = ticker.split(\"/\")\n",
    "                rate = self.currency_rates.get_rate(base_currency, quote_currency)\n",
    "                ticker_data[ticker] = {\"rate\": rate}\n",
    "            else:\n",
    "                # Try fetching data from yfinance first (for stocks)\n",
    "                try:\n",
    "                    ticker_info = yf.Ticker(ticker)\n",
    "                    data = {\n",
    "                        \"current_price\": ticker_info.info.get(\"currentPrice\"),\n",
    "                        \"day_high\": ticker_info.info.get(\"dayHigh\"),\n",
    "                        \"day_low\": ticker_info.info.get(\"dayLow\"),\n",
    "                        \"volume\": ticker_info.info.get(\"volume\"),\n",
    "                        # Add more fields as needed (e.g., 52-week high/low, market cap, etc.)\n",
    "                    }\n",
    "                    ticker_data[ticker] = data\n",
    "                except Exception as e:\n",
    "                    # If yfinance fails, try fetching from CoinGeckoAPI (for crypto)\n",
    "                    try:\n",
    "                        coin_data = self.cg.get_coin_by_id(ticker.lower())\n",
    "                        data = {\n",
    "                            \"current_price\": coin_data[\"market_data\"][\"current_price\"][\n",
    "                                \"usd\"\n",
    "                            ],\n",
    "                            \"day_high\": coin_data[\"market_data\"][\"high_24h\"][\"usd\"],\n",
    "                            \"day_low\": coin_data[\"market_data\"][\"low_24h\"][\"usd\"],\n",
    "                            # Add more fields as needed from coin_data\n",
    "                        }\n",
    "                        ticker_data[ticker] = data\n",
    "                    except Exception as e:\n",
    "                        logging.error(\n",
    "                            f\"Error fetching data for ticker '{ticker}': {str(e)}\"\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching data for ticker '{ticker}': {str(e)}\")\n",
    "        return ticker_data\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze the sentiment of the text using TextBlob.\"\"\"\n",
    "        analysis = TextBlob(text)\n",
    "        return {\n",
    "            \"polarity\": analysis.polarity,\n",
    "            \"subjectivity\": analysis.subjectivity,\n",
    "        }\n",
    "\n",
    "    async def process_portfolio(self):\n",
    "        \"\"\"Process the portfolio and generate blog posts for each ticker.\"\"\"\n",
    "        portfolio = self.read_portfolio()\n",
    "\n",
    "        # Add validation before accessing portfolio data\n",
    "        if not portfolio or \"portfolio\" not in portfolio:\n",
    "            logging.error(\"Invalid portfolio data structure\")\n",
    "            return\n",
    "\n",
    "        for category, tickers in portfolio[\"portfolio\"].items():\n",
    "            output_dir = f\"output/{category}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for ticker in tickers:\n",
    "                output_file = f\"{output_dir}/{ticker}_blog_posts_{datetime.now().strftime('%Y-%m-%d')}.md\"\n",
    "                image_dir = f\"{output_dir}/images\"\n",
    "                os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "                # Calculate date range for the last 7 days\n",
    "                end_date = datetime.now()\n",
    "                start_date = end_date - timedelta(days=self.days_back)\n",
    "\n",
    "                # Perform Tavily client for news articles about the ticker\n",
    "                client_response = self.tavily_client.search(\n",
    "                    query=f\"{ticker} news\",\n",
    "                    from_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    to_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "                )\n",
    "\n",
    "                # Ensure client_response is a dictionary and contains the 'results' key\n",
    "                if (\n",
    "                    not isinstance(client_response, dict)\n",
    "                    or \"results\" not in client_response\n",
    "                ):\n",
    "                    logging.warning(\n",
    "                        f\"Tavily search returned an invalid response for ticker: {ticker}.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                client_results = client_response.get(\"results\", [])\n",
    "\n",
    "                for result in client_results:\n",
    "                    try:\n",
    "                        link = result.get(\"url\", \"\")\n",
    "                        title = result.get(\"title\", \"Untitled\")\n",
    "                        if not link:\n",
    "                            logging.warning(\"Skipping result with missing URL.\")\n",
    "                            continue\n",
    "                        logging.info(f\"Processing article: {title}\")\n",
    "\n",
    "                        article_content = await self.scrape_article_content(link)\n",
    "                        if article_content is None:\n",
    "                            logging.warning(\n",
    "                                f\"Failed to scrape content for article: {title}\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        ticker_data = self.fetch_ticker_data(ticker)\n",
    "                        if not ticker_data:\n",
    "                            logging.warning(\n",
    "                                f\"Failed to fetch ticker data for ticker: {ticker}\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        sentiment = self.analyze_sentiment(article_content)\n",
    "                        if sentiment is None:\n",
    "                            logging.warning(\n",
    "                                f\"Failed to analyze sentiment for article: {title}\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        new_title, new_blog_post = await self.generate_blog_post(\n",
    "                            article_content, ticker, ticker_data, sentiment\n",
    "                        )\n",
    "\n",
    "                        if new_blog_post:\n",
    "                            refined_prompt = self.generate_refined_prompt_for_blog_post(\n",
    "                                {\"content\": article_content}\n",
    "                            )\n",
    "                            image, image_path = self.generate_image(\n",
    "                                refined_prompt, output_dir=image_dir\n",
    "                            )\n",
    "\n",
    "                            clean_title = new_title.strip(\"*\") if new_title else \"\"\n",
    "                            with open(output_file, \"a\") as f:\n",
    "                                f.write(f\"# {clean_title}\\n\\n\")\n",
    "                                f.write(new_blog_post + \"\\n\\n\")\n",
    "                                f.write(f\"Original article: {title}\\n\\n\")\n",
    "                                if image_path:\n",
    "                                    f.write(f\"![Image]({image_path})\\n\\n\")\n",
    "\n",
    "                            logging.info(f\"Generated blog post for {ticker}.\")\n",
    "                        else:\n",
    "                            logging.warning(\n",
    "                                f\"Failed to generate blog post for {ticker}.\"\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing search result: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    blog_generator = AIBlogGenerator()\n",
    "    asyncio.run(blog_generator.process_portfolio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBlogGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        portfolio_file=\"portfolio.json\",\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        tavily_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "    ):\n",
    "        self.portfolio_file = portfolio_file\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "        ]\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        self.exa_client = Exa(api_key=exa_api_key)\n",
    "        self.days_back = 7  # Analyze news from the past 7 days\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.currency_rates = CurrencyRates()\n",
    "        self.cg = CoinGeckoAPI()\n",
    "        self.tavily_api_key = \"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\"\n",
    "        self.tavily_client = TavilyClient(self.tavily_api_key)\n",
    "\n",
    "    def read_portfolio(self) -> Dict[str, List[str]]:\n",
    "        try:\n",
    "            with open(\"portfolio.json\", \"r\") as f:\n",
    "                raw_data = json.load(f)\n",
    "                portfolio = raw_data.get(\n",
    "                    \"portfolio\", {}\n",
    "                )  # Access nested \"portfolio\" key\n",
    "                self.logger.log_operation(\n",
    "                    \"portfolio_loaded\",\n",
    "                    tickers_count=sum(len(v) for v in portfolio.values()),\n",
    "                )\n",
    "                return portfolio\n",
    "        except Exception as e:\n",
    "            self.metrics.error_count.labels(type=\"portfolio_read\").inc()\n",
    "            self.logger.log_operation(\"portfolio_read_failed\", error=str(e))\n",
    "            return {}\n",
    "\n",
    "    def clean_html(self, content):\n",
    "        \"\"\"Remove HTML tags and clean up text.\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            clean_text = \" \".join(soup.stripped_strings)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error cleaning HTML: {str(e)}\")\n",
    "            clean_text = content\n",
    "        return html.unescape(clean_text)\n",
    "\n",
    "    async def scrape_article_content(self, url):\n",
    "        \"\"\"Scrape article content from a URL.\"\"\"\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(self.user_agents),\n",
    "            \"Accept\": \"text/html\",\n",
    "        }\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(url, headers=headers) as response:\n",
    "                    logging.info(f\"Response headers for {url}: {response.headers}\")\n",
    "                    response.raise_for_status()\n",
    "                    elements = partition_html(text=await response.text())\n",
    "                    content = \" \".join([str(element) for element in elements])\n",
    "                    clean_content = self.clean_html(content)\n",
    "                    logging.info(f\"Scraped content for {url}: {clean_content[:500]}...\")\n",
    "                    return clean_content\n",
    "        except aiohttp.ClientResponseError as e:\n",
    "            logging.error(\n",
    "                f\"Error scraping {url}: {e.status}, message='{e.message}', url='{e.request_info.url}'\"\n",
    "            )\n",
    "            return \"Unable to scrape content\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return \"Unable to scrape content\"\n",
    "\n",
    "    async def generate_blog_post(\n",
    "        self, article_content, ticker, ticker_data=None, sentiment=None\n",
    "    ):\n",
    "        \"\"\"Generate a blog post using AI.\"\"\"\n",
    "\n",
    "        if ticker_data and sentiment:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, ticker data, and sentiment analysis, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data and sentiment analysis, where appropriate.\n",
    "            Finally, suggest a market action (buy, sell, or hold) for {ticker} based on the analysis.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            Sentiment Analysis: {sentiment}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        elif ticker_data:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content and ticker data, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data, where appropriate.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, write a new blog post about {ticker}. The new post should be \n",
    "            original, engaging, and informative, while capturing the key points and insights from \n",
    "            the source material.\n",
    "            Source content: {article_content[:4000]} \n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"llama-3.2-3b-preview\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a professional blogger who writes engaging and informative content across various topics, including technology, lifestyle, health, and culture. Adapt your tone and style to suit each subject while ensuring your writing is relatable and thought-provoking. When suggesting market actions, be cautious and avoid making strong recommendations. Use phrases like 'might be a good time to consider', 'could be an opportunity to', or 'investors may want to watch' to express potential actions without giving direct financial advice.\",\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "                if response.choices and response.choices[0].message.content:\n",
    "                    blog_post_content = response.choices[0].message.content.strip()\n",
    "\n",
    "                    # Assuming the new title is returned at the beginning of the content\n",
    "                    title = (\n",
    "                        blog_post_content.split(\"\\n\")[0] if blog_post_content else \"\"\n",
    "                    )  # Handle potential None\n",
    "                    new_blog_post = (\n",
    "                        \"\\n\".join(blog_post_content.split(\"\\n\")[1:])\n",
    "                        if blog_post_content\n",
    "                        else \"\"\n",
    "                    )  # Handle potential None\n",
    "\n",
    "                    return (\n",
    "                        title,\n",
    "                        new_blog_post,\n",
    "                    )  # Return both title and content\n",
    "                else:\n",
    "                    logging.error(\"OpenAI response is empty or incomplete.\")\n",
    "                    return None, None\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    wait_time = 2**attempt  # Exponential backoff\n",
    "                    logging.warning(\n",
    "                        f\"Rate limit exceeded. Waiting for {wait_time} seconds.\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logging.error(f\"Error generating blog post: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "    def find_similar_content(self, url, max_results=5):\n",
    "        \"\"\"Find similar articles using the Exa API.\"\"\"\n",
    "        try:\n",
    "            response = self.exa_client.find_similar_and_contents(\n",
    "                url, text=True, num_results=max_results\n",
    "            )\n",
    "            similar_articles = []\n",
    "            for result in response.results:\n",
    "                similar_articles.append(\n",
    "                    {\n",
    "                        \"title\": result.title,\n",
    "                        \"url\": result.url,\n",
    "                        \"text\": result.text if hasattr(result, \"text\") else None,\n",
    "                    }\n",
    "                )\n",
    "            return similar_articles\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error finding similar content for URL '{url}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt,\n",
    "        output_dir=\".\",\n",
    "        width=1024,\n",
    "        height=768,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=9,\n",
    "        scheduler=\"heunpp2\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate an image using the Hugging Face FLUX.1-dev model.\"\"\"\n",
    "        API_URL = (\n",
    "            \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "        )\n",
    "        API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "        headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"scheduler\": scheduler,\n",
    "                **kwargs,  # Pass additional parameters here\n",
    "            },\n",
    "        }\n",
    "\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                image = Image.open(io.BytesIO(response.content))\n",
    "                timestamp = int(time.time())\n",
    "                filename = f\"image_{timestamp}.png\"\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                image.save(output_path)\n",
    "                return image, output_path\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing image data: {str(e)}\")\n",
    "                return None, None\n",
    "        else:\n",
    "            logging.error(f\"Image generation API failed: {response.text}\")\n",
    "            return None, None\n",
    "\n",
    "    def extract_nouns(self, text):\n",
    "        \"\"\"Extract unique nouns from text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        return list(set(nouns))  # Return unique nouns\n",
    "\n",
    "    def generate_refined_prompt_for_blog_post(self, post):\n",
    "        \"\"\"Generate a refined prompt for AI image generation.\"\"\"\n",
    "        summary = self.summarize_with_spacy(post[\"content\"])\n",
    "        nouns = self.extract_nouns(summary)\n",
    "        key_elements = \", \".join(nouns[:7])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Create a visually-rich prompt for AI image generation based on the summary of this blog post:\n",
    "\n",
    "        {summary}\n",
    "\n",
    "        Key visual themes: {key_elements}\n",
    "\n",
    "        The prompt should:\n",
    "        1. Be concise and focused on visual elements.\n",
    "        2. Seamlessly incorporate key visual concepts without explicitly listing them.\n",
    "        3. Maintain a professional tone inline with {summary} .\n",
    "        4. Be optimized for AI image generation models, emphasizing clarity and detail.\n",
    "\n",
    "        Please provide only the refined prompt without any additional commentary or explanation. The example should serve as inspiration without replicating its structure or content directly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at creating prompts for AI image generation.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=150,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            return (\n",
    "                response.choices[0].message.content.strip()\n",
    "                if response.choices[0].message.content\n",
    "                else \"A modern illustration representing key themes from the blog post.\"\n",
    "            )  # Handle potential None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating refined prompt: {str(e)}\")\n",
    "            return \"A modern illustration representing key themes from the blog post.\"\n",
    "\n",
    "    def summarize_with_spacy(self, text):\n",
    "        \"\"\"Summarize text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]\n",
    "        return \" \".join(sentences[:7])\n",
    "\n",
    "    def extract_tickers(self, text):\n",
    "        \"\"\"Extract potential stock tickers from text using regular expressions.\"\"\"\n",
    "        ticker_pattern = r\"\\b([A-Z]{2,}[A-Z0-9\\.\\-]*|[A-Z]{3,}/[A-Z]{3,})\\b\"\n",
    "        tickers = re.findall(ticker_pattern, text)\n",
    "        return list(set(tickers))  # Return unique tickers\n",
    "\n",
    "    def fetch_ticker_data(self, ticker):\n",
    "        \"\"\"Fetch relevant ticker data using yfinance and CoinGeckoAPI.\"\"\"\n",
    "        ticker_data = {}\n",
    "        try:\n",
    "            # Check if it's a forex pair (e.g., EUR/USD)\n",
    "            if \"/\" in ticker:\n",
    "                base_currency, quote_currency = ticker.split(\"/\")\n",
    "                rate = self.currency_rates.get_rate(base_currency, quote_currency)\n",
    "                ticker_data[ticker] = {\"rate\": rate}\n",
    "            else:\n",
    "                # Try fetching data from yfinance first (for stocks)\n",
    "                try:\n",
    "                    ticker_info = yf.Ticker(ticker)\n",
    "                    data = {\n",
    "                        \"current_price\": ticker_info.info.get(\"currentPrice\"),\n",
    "                        \"day_high\": ticker_info.info.get(\"dayHigh\"),\n",
    "                        \"day_low\": ticker_info.info.get(\"dayLow\"),\n",
    "                        \"volume\": ticker_info.info.get(\"volume\"),\n",
    "                        \"market_cap\": ticker_info.info.get(\"marketCap\"),\n",
    "                        # Add more fields as needed (e.g., 52-week high/low, market cap, etc.)\n",
    "                    }\n",
    "                    ticker_data[ticker] = data\n",
    "                except Exception as e:\n",
    "                    # If yfinance fails, try fetching from CoinGeckoAPI (for crypto)\n",
    "                    try:\n",
    "                        coin_data = self.cg.get_coin_by_id(ticker.lower())\n",
    "                        data = {\n",
    "                            \"current_price\": coin_data[\"market_data\"][\"current_price\"][\n",
    "                                \"usd\"\n",
    "                            ],\n",
    "                            \"day_high\": coin_data[\"market_data\"][\"high_24h\"][\"usd\"],\n",
    "                            \"day_low\": coin_data[\"market_data\"][\"low_24h\"][\"usd\"],\n",
    "                            \"volume\": coin_data[\"market_data\"][\"total_volume\"][\"usd\"],\n",
    "                            \"market_cap\": coin_data[\"market_data\"][\"market_cap\"][\"usd\"],\n",
    "                            # Add more fields as needed from coin_data\n",
    "                        }\n",
    "                        ticker_data[ticker] = data\n",
    "                    except Exception as e:\n",
    "                        logging.error(\n",
    "                            f\"Error fetching data for ticker '{ticker}' from CoinGeckoAPI: {str(e)}\"\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching data for ticker '{ticker}': {str(e)}\")\n",
    "        return ticker_data\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze the sentiment of the text using TextBlob.\"\"\"\n",
    "        analysis = TextBlob(text)\n",
    "        return {\n",
    "            \"polarity\": analysis.polarity,\n",
    "            \"subjectivity\": analysis.subjectivity,\n",
    "        }\n",
    "\n",
    "    async def process_portfolio(self):\n",
    "        \"\"\"Process the portfolio and generate blog posts for each ticker.\"\"\"\n",
    "        portfolio = self.read_portfolio()\n",
    "\n",
    "        # Add validation before accessing portfolio data\n",
    "        if not portfolio or \"portfolio\" not in portfolio:\n",
    "            logging.error(\"Invalid portfolio data structure\")\n",
    "            return\n",
    "\n",
    "        for category, tickers in portfolio[\"portfolio\"].items():\n",
    "            output_dir = f\"output/{category}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for ticker in tickers:\n",
    "                output_file = f\"{output_dir}/{ticker}_blog_posts_{datetime.now().strftime('%Y-%m-%d')}.md\"\n",
    "                image_dir = f\"{output_dir}/images\"\n",
    "                os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "                # Calculate date range for the last 7 days\n",
    "                end_date = datetime.now()\n",
    "                start_date = end_date - timedelta(days=self.days_back)\n",
    "\n",
    "                # Perform Tavily client for news articles about the ticker\n",
    "                client_response = self.tavily_client.search(\n",
    "                    query=f\"Latest {ticker} news\",\n",
    "                    topic=\"news\",\n",
    "                    include_raw_content=True,\n",
    "                    days=self.days_back,\n",
    "                    from_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    to_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "                )\n",
    "\n",
    "                # Ensure client_response is a dictionary and contains the 'results' key\n",
    "                if (\n",
    "                    not isinstance(client_response, dict)\n",
    "                    or \"results\" not in client_response\n",
    "                ):\n",
    "                    logging.warning(\n",
    "                        f\"Tavily search returned an invalid response for ticker: {ticker}.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                client_results = client_response.get(\"results\", [])\n",
    "\n",
    "                all_article_content = []\n",
    "\n",
    "                for result in client_results:\n",
    "                    try:\n",
    "                        link = result.get(\"url\", \"\")\n",
    "                        title = result.get(\"title\", \"Untitled\")\n",
    "                        if not link:\n",
    "                            logging.warning(\"Skipping result with missing URL.\")\n",
    "                            continue\n",
    "                        logging.info(f\"Processing article: {title}\")\n",
    "\n",
    "                        article_content = await self.scrape_article_content(link)\n",
    "                        if article_content is None:\n",
    "                            logging.warning(\n",
    "                                f\"Failed to scrape content for article: {title}\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                        all_article_content.append(article_content)\n",
    "\n",
    "                        # Find similar content for the article\n",
    "                        similar_articles = self.find_similar_content(link)\n",
    "                        for similar_article in similar_articles:\n",
    "                            similar_content = await self.scrape_article_content(\n",
    "                                similar_article[\"url\"]\n",
    "                            )\n",
    "                            if similar_content:\n",
    "                                all_article_content.append(similar_content)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing search result: {str(e)}\")\n",
    "\n",
    "                if all_article_content:\n",
    "                    combined_content = \"\\n\\n\".join(all_article_content)\n",
    "                    ticker_data = self.fetch_ticker_data(ticker)\n",
    "                    sentiment = self.analyze_sentiment(combined_content)\n",
    "\n",
    "                    new_title, new_blog_post = await self.generate_blog_post(\n",
    "                        combined_content, ticker, ticker_data, sentiment\n",
    "                    )\n",
    "\n",
    "                    if new_blog_post:\n",
    "                        refined_prompt = self.generate_refined_prompt_for_blog_post(\n",
    "                            {\"content\": combined_content}\n",
    "                        )\n",
    "                        image, image_path = self.generate_image(\n",
    "                            refined_prompt, output_dir=image_dir\n",
    "                        )\n",
    "\n",
    "                        clean_title = new_title.strip(\"*\") if new_title else \"\"\n",
    "                        with open(output_file, \"a\") as f:\n",
    "                            f.write(f\"# {clean_title}\\n\\n\")\n",
    "                            f.write(new_blog_post + \"\\n\\n\")\n",
    "                            if image_path:\n",
    "                                f.write(f\"![Image]({image_path})\\n\\n\")\n",
    "\n",
    "                        logging.info(f\"Generated blog post for {ticker}.\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Failed to generate blog post for {ticker}.\")\n",
    "                else:\n",
    "                    logging.warning(f\"No article content found for {ticker}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    blog_generator = AIBlogGenerator()\n",
    "    asyncio.run(blog_generator.process_portfolio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tavily API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBlogGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        portfolio_file=\"portfolio.json\",\n",
    "        tavily_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "    ):\n",
    "        self.portfolio_file = portfolio_file\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "        ]\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        self.days_back = 7  # Analyze news from the past 7 days\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.currency_rates = CurrencyRates()\n",
    "        self.cg = CoinGeckoAPI()\n",
    "        self.tavily_api_key = \"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\"\n",
    "        self.tavily_client = TavilyClient(self.tavily_api_key)\n",
    "\n",
    "    def read_portfolio(self):\n",
    "        \"\"\"Read portfolio data from a JSON file.\"\"\"\n",
    "        with open(self.portfolio_file, \"r\") as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def clean_html(self, content):\n",
    "        \"\"\"Remove HTML tags and clean up text.\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            clean_text = \" \".join(soup.stripped_strings)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error cleaning HTML: {str(e)}\")\n",
    "            clean_text = content\n",
    "        return html.unescape(clean_text)\n",
    "\n",
    "    async def generate_blog_post(\n",
    "        self, article_content, ticker, ticker_data=None, sentiment=None\n",
    "    ):\n",
    "        \"\"\"Generate a blog post using AI.\"\"\"\n",
    "\n",
    "        if ticker_data and sentiment:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, ticker data, and sentiment analysis, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data and sentiment analysis, where appropriate.\n",
    "            Finally, suggest a market action (buy, sell, or hold) for {ticker} based on the analysis.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            Sentiment Analysis: {sentiment}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        elif ticker_data:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content and ticker data, write a new blog post about {ticker}. \n",
    "            The new post should be original, engaging, and informative, while capturing the key \n",
    "            points and insights from the source material. It should also incorporate relevant \n",
    "            insights and analysis from the ticker data, where appropriate.\n",
    "\n",
    "            Source content: {article_content[:4000]} \n",
    "\n",
    "            Ticker Data: {ticker_data}\n",
    "\n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following article content, write a new blog post about {ticker}. The new post should be \n",
    "            original, engaging, and informative, while capturing the key points and insights from \n",
    "            the source material.\n",
    "            Source content: {article_content[:4000]} \n",
    "            New blog post:\n",
    "            \"\"\"\n",
    "\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"llama-3.2-3b-preview\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a professional blogger who writes engaging and informative content across various topics, including technology, lifestyle, health, and culture. Adapt your tone and style to suit each subject while ensuring your writing is relatable and thought-provoking. When suggesting market actions, be cautious and avoid making strong recommendations. Use phrases like 'might be a good time to consider', 'could be an opportunity to', or 'investors may want to watch' to express potential actions without giving direct financial advice.\",\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "                if response.choices and response.choices[0].message.content:\n",
    "                    blog_post_content = response.choices[0].message.content.strip()\n",
    "\n",
    "                    # Assuming the new title is returned at the beginning of the content\n",
    "                    title = (\n",
    "                        blog_post_content.split(\"\\n\")[0] if blog_post_content else \"\"\n",
    "                    )  # Handle potential None\n",
    "                    new_blog_post = (\n",
    "                        \"\\n\".join(blog_post_content.split(\"\\n\")[1:])\n",
    "                        if blog_post_content\n",
    "                        else \"\"\n",
    "                    )  # Handle potential None\n",
    "\n",
    "                    return (\n",
    "                        title,\n",
    "                        new_blog_post,\n",
    "                    )  # Return both title and content\n",
    "                else:\n",
    "                    logging.error(\"OpenAI response is empty or incomplete.\")\n",
    "                    return None, None\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    wait_time = 2**attempt  # Exponential backoff\n",
    "                    logging.warning(\n",
    "                        f\"Rate limit exceeded. Waiting for {wait_time} seconds.\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logging.error(f\"Error generating blog post: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt,\n",
    "        output_dir=\".\",\n",
    "        width=1024,\n",
    "        height=768,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=9,\n",
    "        scheduler=\"heunpp2\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate an image using the Hugging Face FLUX.1-dev model.\"\"\"\n",
    "        API_URL = (\n",
    "            \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "        )\n",
    "        API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "        headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"num_inference_steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"scheduler\": scheduler,\n",
    "                **kwargs,  # Pass additional parameters here\n",
    "            },\n",
    "        }\n",
    "\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(API_URL, headers=headers, json=payload)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    try:\n",
    "                        image = Image.open(io.BytesIO(response.content))\n",
    "                        timestamp = int(time.time())\n",
    "                        filename = f\"image_{timestamp}.png\"\n",
    "                        output_path = os.path.join(output_dir, filename)\n",
    "                        image.save(output_path)\n",
    "                        return image, output_path\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing image data: {str(e)}\")\n",
    "                        return None, None\n",
    "                else:\n",
    "                    logging.error(f\"Image generation API failed: {response.text}\")\n",
    "                    return None, None\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if \"Model too busy\" in str(e):\n",
    "                    wait_time = 2**attempt  # Exponential backoff\n",
    "                    logging.warning(\n",
    "                        f\"Model too busy. Waiting for {wait_time} seconds before retrying.\"\n",
    "                    )\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logging.error(f\"Error generating image: {str(e)}\")\n",
    "                    return None, None\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating image: {str(e)}\")\n",
    "                return None, None\n",
    "\n",
    "        logging.error(\"Failed to generate image after multiple retries.\")\n",
    "        return None, None\n",
    "\n",
    "    def extract_nouns(self, text):\n",
    "        \"\"\"Extract unique nouns from text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        return list(set(nouns))  # Return unique nouns\n",
    "\n",
    "    def generate_refined_prompt_for_blog_post(self, post):\n",
    "        \"\"\"Generate a refined prompt for AI image generation.\"\"\"\n",
    "        summary = self.summarize_with_spacy(post[\"content\"])\n",
    "        nouns = self.extract_nouns(summary)\n",
    "        key_elements = \", \".join(nouns[:7])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Create a visually-rich prompt for AI image generation based on the summary of this blog post:\n",
    "\n",
    "        {summary}\n",
    "\n",
    "        Key visual themes: {key_elements}\n",
    "\n",
    "        The prompt should:\n",
    "        1. Be concise and focused on visual elements.\n",
    "        2. Seamlessly incorporate key visual concepts without explicitly listing them.\n",
    "        3. Maintain a professional tone inline with {summary} .\n",
    "        4. Be optimized for AI image generation models, emphasizing clarity and detail.\n",
    "\n",
    "        Please provide only the refined prompt without any additional commentary or explanation. The example should serve as inspiration without replicating its structure or content directly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at creating prompts for AI image generation.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=100,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            return (\n",
    "                response.choices[0].message.content.strip()\n",
    "                if response.choices[0].message.content\n",
    "                else \"A modern illustration representing key themes from the blog post.\"\n",
    "            )  # Handle potential None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating refined prompt: {str(e)}\")\n",
    "            return \"A modern illustration representing key themes from the blog post.\"\n",
    "\n",
    "    def summarize_with_spacy(self, text):\n",
    "        \"\"\"Summarize text using spaCy.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]\n",
    "        return \" \".join(sentences[:7])\n",
    "\n",
    "    def extract_tickers(self, text):\n",
    "        \"\"\"Extract potential stock tickers from text using regular expressions.\"\"\"\n",
    "        ticker_pattern = r\"\\b([A-Z]{2,}[A-Z0-9\\.\\-]*|[A-Z]{3,}/[A-Z]{3,})\\b\"\n",
    "        tickers = re.findall(ticker_pattern, text)\n",
    "        return list(set(tickers))  # Return unique tickers\n",
    "\n",
    "    def fetch_ticker_data(self, ticker):\n",
    "        \"\"\"Fetch relevant ticker data using yfinance and CoinGeckoAPI.\"\"\"\n",
    "        ticker_data = {}\n",
    "        try:\n",
    "            # Check if it's a forex pair (e.g., EUR/USD)\n",
    "            if \"/\" in ticker:\n",
    "                base_currency, quote_currency = ticker.split(\"/\")\n",
    "                rate = self.currency_rates.get_rate(base_currency, quote_currency)\n",
    "                ticker_data[ticker] = {\"rate\": rate}\n",
    "            else:\n",
    "                # Try fetching data from yfinance first (for stocks)\n",
    "                try:\n",
    "                    ticker_info = yf.Ticker(ticker)\n",
    "                    data = {\n",
    "                        \"current_price\": ticker_info.info.get(\"currentPrice\"),\n",
    "                        \"day_high\": ticker_info.info.get(\"dayHigh\"),\n",
    "                        \"day_low\": ticker_info.info.get(\"dayLow\"),\n",
    "                        \"volume\": ticker_info.info.get(\"volume\"),\n",
    "                        \"market_cap\": ticker_info.info.get(\"marketCap\"),\n",
    "                        # Add more fields as needed (e.g., 52-week high/low, market cap, etc.)\n",
    "                    }\n",
    "                    ticker_data[ticker] = data\n",
    "                except Exception as e:\n",
    "                    # If yfinance fails, try fetching from CoinGeckoAPI (for crypto)\n",
    "                    try:\n",
    "                        coin_data = self.cg.get_coin_by_id(ticker.lower())\n",
    "                        data = {\n",
    "                            \"current_price\": coin_data[\"market_data\"][\"current_price\"][\n",
    "                                \"usd\"\n",
    "                            ],\n",
    "                            \"day_high\": coin_data[\"market_data\"][\"high_24h\"][\"usd\"],\n",
    "                            \"day_low\": coin_data[\"market_data\"][\"low_24h\"][\"usd\"],\n",
    "                            \"volume\": coin_data[\"market_data\"][\"total_volume\"][\"usd\"],\n",
    "                            \"market_cap\": coin_data[\"market_data\"][\"market_cap\"][\"usd\"],\n",
    "                            # Add more fields as needed from coin_data\n",
    "                        }\n",
    "                        ticker_data[ticker] = data\n",
    "                    except Exception as e:\n",
    "                        logging.error(\n",
    "                            f\"Error fetching data for ticker '{ticker}': {str(e)}\"\n",
    "                        )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching data for ticker '{ticker}': {str(e)}\")\n",
    "        return ticker_data\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze the sentiment of the text using TextBlob.\"\"\"\n",
    "        analysis = TextBlob(text)\n",
    "        return {\n",
    "            \"polarity\": analysis.polarity,\n",
    "            \"subjectivity\": analysis.subjectivity,\n",
    "        }\n",
    "\n",
    "    async def process_portfolio(self):\n",
    "        \"\"\"Process the portfolio and generate blog posts for each ticker.\"\"\"\n",
    "        portfolio = self.read_portfolio()\n",
    "\n",
    "        # Add validation before accessing portfolio data\n",
    "        if not portfolio or \"portfolio\" not in portfolio:\n",
    "            logging.error(\"Invalid portfolio data structure\")\n",
    "            return\n",
    "\n",
    "        for category, tickers in portfolio[\"portfolio\"].items():\n",
    "            output_dir = f\"output/{category}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for ticker in tickers:\n",
    "                output_file = f\"{output_dir}/{ticker}_blog_posts_{datetime.now().strftime('%Y-%m-%d')}.md\"\n",
    "                image_dir = f\"{output_dir}/images\"\n",
    "                os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "                # Calculate date range for the last 7 days\n",
    "                end_date = datetime.now()\n",
    "                start_date = end_date - timedelta(days=self.days_back)\n",
    "\n",
    "                # Perform Tavily client for news articles about the ticker\n",
    "                client_response = self.tavily_client.search(\n",
    "                    query=f\"Latest {ticker} news and analysis\",\n",
    "                    from_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "                    to_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "                )\n",
    "\n",
    "                # Ensure client_response is a dictionary and contains the 'results' key\n",
    "                if (\n",
    "                    not isinstance(client_response, dict)\n",
    "                    or \"results\" not in client_response\n",
    "                ):\n",
    "                    logging.warning(\n",
    "                        f\"Tavily search returned an invalid response for ticker: {ticker}.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                client_results = client_response.get(\"results\", [])\n",
    "\n",
    "                all_article_content = []\n",
    "\n",
    "                for result in client_results:\n",
    "                    try:\n",
    "                        link = result.get(\"url\", \"\")\n",
    "                        title = result.get(\"title\", \"Untitled\")\n",
    "                        if not link:\n",
    "                            logging.warning(\"Skipping result with missing URL.\")\n",
    "                            continue\n",
    "                        logging.info(f\"Processing article: {title}\")\n",
    "\n",
    "                        # Extract raw content using Tavily API\n",
    "                        extract_response = self.tavily_client.extract(urls=[link])\n",
    "                        if extract_response and \"results\" in extract_response:\n",
    "                            for extract_result in extract_response[\"results\"]:\n",
    "                                raw_content = extract_result.get(\"raw_content\", \"\")\n",
    "                                all_article_content.append(raw_content)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing search result: {str(e)}\")\n",
    "\n",
    "                if all_article_content:\n",
    "                    combined_content = \"\\n\\n\".join(all_article_content)\n",
    "                    ticker_data = self.fetch_ticker_data(ticker)\n",
    "                    sentiment = self.analyze_sentiment(combined_content)\n",
    "\n",
    "                    new_title, new_blog_post = await self.generate_blog_post(\n",
    "                        combined_content, ticker, ticker_data, sentiment\n",
    "                    )\n",
    "\n",
    "                    if new_blog_post:\n",
    "                        refined_prompt = self.generate_refined_prompt_for_blog_post(\n",
    "                            {\"content\": combined_content}\n",
    "                        )\n",
    "                        image, image_path = self.generate_image(\n",
    "                            refined_prompt, output_dir=image_dir\n",
    "                        )\n",
    "\n",
    "                        clean_title = new_title.strip(\"*\") if new_title else \"\"\n",
    "                        with open(output_file, \"a\") as f:\n",
    "                            f.write(f\"# {clean_title}\\n\\n\")\n",
    "                            f.write(new_blog_post + \"\\n\\n\")\n",
    "                            if image_path:\n",
    "                                f.write(f\"![Image]({image_path})\\n\\n\")\n",
    "\n",
    "                        logging.info(f\"Generated insight for {ticker}.\")\n",
    "                    else:\n",
    "                        logging.warning(f\"Failed to generate insight for {ticker}.\")\n",
    "                else:\n",
    "                    logging.warning(f\"No article content found for {ticker}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    blog_generator = AIBlogGenerator()\n",
    "    asyncio.run(blog_generator.process_portfolio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import spacy\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "import html\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from forex_python.converter import CurrencyRates\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "from textblob import TextBlob\n",
    "import yfinance as yf\n",
    "from tavily import TavilyClient\n",
    "from redis import Redis\n",
    "from prometheus_client import Counter, Histogram, start_http_server\n",
    "import structlog\n",
    "from pydantic_settings import BaseSettings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "import uvicorn\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "This class defines the application settings for the project. It inherits from the `BaseSettings` class from the `pydantic_settings` library, which allows for easy loading of environment variables.\n",
    "    \n",
    "    The settings include:\n",
    "    - `GROQ_API_KEY`: The API key for the GROQ API\n",
    "    - `TAVILY_API_KEY`: The API key for the Tavily API\n",
    "    - `HF_API_KEY`: The API key for the Hugging Face API\n",
    "    - `REDIS_URL`: The URL for the Redis server\n",
    "    - `PROMETHEUS_PORT`: The port for the Prometheus metrics server (default is 8000)\n",
    "    - `LOG_LEVEL`: The logging level for the application (default is \"INFO\")\n",
    "    - `BATCH_SIZE`: The batch size for processing data (default is 10)\n",
    "    - `CACHE_TTL`: The time-to-live for cached data (default is 3600 seconds, or 1 hour)\n",
    "    \n",
    "    The `Config` class specifies that the environment variables should be loaded from a `.env` file.\n",
    "        GROQ_API_KEY: str\n",
    "    TAVILY_API_KEY: str\n",
    "    HF_API_KEY: str\n",
    "    REDIS_URL: str\n",
    "    PROMETHEUS_PORT: int = 8000\n",
    "    LOG_LEVEL: str = \"INFO\"\n",
    "    BATCH_SIZE: int = 10\n",
    "    CACHE_TTL: int = 3600\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "The `MetricsCollector` class is responsible for collecting and managing various metrics for the application. It provides methods for recording API requests, processing time, and errors. The metrics collected are intended to be used for monitoring and observability purposes.\n",
    "class MetricsCollector:\n",
    "    def __init__(self):\n",
    "        self.api_requests = Counter('api_requests_total', 'Total API requests', ['endpoint'])\n",
    "        self.processing_time = Histogram('processing_duration_seconds', 'Time spent processing')\n",
    "        self.error_count = Counter('errors_total', 'Total errors', ['type'])\n",
    "\n",
    "The `StructuredLogger` class is responsible for providing a structured logging interface for the application. It uses the `structlog` library to create a logger instance and provides a `log_operation` method to log information about various operations performed by the application.\n",
    "class StructuredLogger:\n",
    "    def __init__(self):\n",
    "        self.logger = structlog.get_logger()\n",
    "        \n",
    "    def log_operation(self, operation: str, **kwargs):\n",
    "        return self.logger.info(operation, **kwargs)\n",
    "\n",
    "The `CacheManager` class is responsible for managing a cache using a Redis server. It provides methods for fetching data from the cache or setting new data in the cache with a configurable time-to-live (TTL). The class takes a Redis URL as input and uses the `redis-py` library to interact with the Redis server.\n",
    "class CacheManager:\n",
    "    def __init__(self, redis_url: str):\n",
    "        self.redis = Redis.from_url(redis_url)\n",
    "        self.cache_ttl = 3600\n",
    "\n",
    "    async def get_or_set(self, key: str, fetch_func, ttl: Optional[int] = None):\n",
    "        if cached := self.redis.get(key):\n",
    "            return cached\n",
    "        value = await fetch_func()\n",
    "        self.redis.set(key, value, ex=ttl or self.cache_ttl)\n",
    "        return value\n",
    "\n",
    "The `APIClient` class is responsible for making API requests with retry logic. It takes a `ClientSession` and `Settings` object as input, and provides a `fetch_with_retry` method that will make up to 3 attempts to fetch data from a given URL, with exponential backoff between attempts.\n",
    "class APIClient:\n",
    "    def __init__(self, session: aiohttp.ClientSession, config: Settings):\n",
    "        self.session = session\n",
    "        self.config = config\n",
    "        self._rate_limit_delay = 0.1\n",
    "    \n",
    "    async def fetch_with_retry(self, url: str, **kwargs):\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                async with self.session.request(**kwargs) as response:\n",
    "                    return await response.json()\n",
    "            except Exception as e:\n",
    "                if attempt == 2:\n",
    "                    raise\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "\n",
    "The `ImageService` class is responsible for managing the generation of images using an external API. It takes an `APIClient` instance and a `MetricsCollector` instance as dependencies, which are used to make API requests and collect metrics about the image generation process, respectively.\n",
    "class ImageService:\n",
    "    def __init__(self, api_client: APIClient, metrics: MetricsCollector):\n",
    "        self.api_client = api_client\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    async def generate_image(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_dir: str,\n",
    "        width: int = 1024,\n",
    "        height: int = 768,\n",
    "    ) -> Tuple[Optional[Image.Image], Optional[str]]:\n",
    "        with self.metrics.processing_time.time():\n",
    "            try:\n",
    "                response = await self.api_client.fetch_with_retry(\n",
    "                    \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\",\n",
    "                    method=\"POST\",\n",
    "                    json={\"inputs\": prompt}\n",
    "                )\n",
    "                image = Image.open(io.BytesIO(response.content))\n",
    "                timestamp = int(time.time())\n",
    "                filename = f\"image_{timestamp}.png\"\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                image.save(output_path)\n",
    "                return image, output_path\n",
    "            except Exception as e:\n",
    "                self.metrics.error_count.labels(type='image_generation').inc()\n",
    "                return None, None\n",
    "\n",
    "The `ContentGenerator` class is responsible for generating content, such as blog posts, using an AI-powered content generation service. It takes an `OpenAI` client, a `MetricsCollector`, and a `CacheManager` as dependencies, and provides a `generate_content` method that generates content for a given article, ticker, ticker data, and sentiment.\n",
    "class ContentGenerator:\n",
    "    def __init__(self, client: OpenAI, metrics: MetricsCollector, cache: CacheManager):\n",
    "        self.client = client\n",
    "        self.metrics = metrics\n",
    "        self.cache = cache\n",
    "        \n",
    "    async def generate_content(\n",
    "        self,\n",
    "        article_content: str,\n",
    "        ticker: str,\n",
    "        ticker_data: Dict,\n",
    "        sentiment: Any\n",
    "    ) -> Tuple[str, str]:\n",
    "        cache_key = f\"content_{hash(article_content)}\"\n",
    "        \n",
    "        async def generate():\n",
    "            with self.metrics.processing_time.time():\n",
    "                response = await self._get_ai_response(\n",
    "                    article_content, ticker, ticker_data, sentiment\n",
    "                )\n",
    "                return self._process_response(response)\n",
    "                \n",
    "        return await self.cache.get_or_set(cache_key, generate)\n",
    "\n",
    "Represents a class that generates complete blog posts, including content, images, and other necessary components.\n",
    "class CompleteBlogGenerator:\n",
    "    def __init__(self):\n",
    "        self.config = Settings()\n",
    "        self.setup_components()\n",
    "        \n",
    "    def setup_components(self):\n",
    "        self.metrics = MetricsCollector()\n",
    "        self.logger = StructuredLogger()\n",
    "        self.cache = CacheManager(self.config.REDIS_URL)\n",
    "        self.setup_services()\n",
    "        \n",
    "    def setup_services(self):\n",
    "        self.services = {\n",
    "            'data_pipeline': DataPipeline(self.services, self.config),\n",
    "            'sentiment': EnhancedSentimentAnalyzer(self.metrics),\n",
    "            'image': ImageService(self.api_client, self.metrics),\n",
    "            'content': ContentGenerator(self.client, self.metrics, self.cache),\n",
    "            'portfolio': PortfolioProcessor(self.services, self.metrics, self.logger)\n",
    "        }\n",
    "        \n",
    "    async def process_ticker(self, category: str, ticker: str):\n",
    "        try:\n",
    "            data = await self.services['data_pipeline'].process_data(ticker)\n",
    "            content = await self.services['content'].generate_content(\n",
    "                data['articles'],\n",
    "                ticker,\n",
    "                data['ticker_data'],\n",
    "                data['sentiment_data']\n",
    "            )\n",
    "            \n",
    "            image_result = await self.services['image'].generate_image(\n",
    "                content['title'],\n",
    "                f\"output/{category}/images\"\n",
    "            )\n",
    "            \n",
    "            await self.output_manager.save_blog_post(\n",
    "                category,\n",
    "                ticker,\n",
    "                {**content, 'image_path': image_result[1]}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.error_count.labels(type='process_ticker').inc()\n",
    "            self.logger.log_operation('ticker_processing_failed', error=str(e))\n",
    "            \n",
    "    async def run(self):\n",
    "        portfolio = self.read_portfolio()\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [\n",
    "                self.process_ticker(category, ticker)\n",
    "                for category, tickers in portfolio['portfolio'].items()\n",
    "                for ticker in tickers\n",
    "            ]\n",
    "            await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = CompleteBlogGenerator()\n",
    "    asyncio.run(generator.run())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-26 11:22:43\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mportfolio_loaded              \u001b[0m \u001b[36mtickers_count\u001b[0m=\u001b[35m16\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API request failed with status 400\n",
      "ERROR:root:API request failed with status 400\n",
      "ERROR:root:API request failed with status 400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import spacy\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "import html\n",
    "import yfinance as yf\n",
    "import structlog\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import prometheus_client\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from forex_python.converter import CurrencyRates\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "from textblob import TextBlob\n",
    "from tavily import TavilyClient\n",
    "from redis import Redis\n",
    "from prometheus_client import Counter, Histogram, start_http_server, core\n",
    "from pydantic_settings import BaseSettings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    GROQ_API_KEY: str\n",
    "    TAVILY_API_KEY: str\n",
    "    HF_API_TOKEN: str\n",
    "    DEBUG_LEVEL: str = \"DEBUG\"\n",
    "    REDIS_URL: str\n",
    "    PROMETHEUS_PORT: int = 8000\n",
    "    BATCH_SIZE: int = 10\n",
    "    CACHE_TTL: int = 3600\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "\n",
    "class StructuredLogger:\n",
    "    def __init__(self):\n",
    "        self.logger = structlog.get_logger()\n",
    "\n",
    "    def log_operation(self, operation: str, **kwargs):\n",
    "        return self.logger.info(operation, **kwargs)\n",
    "\n",
    "\n",
    "class CacheManager:\n",
    "    def __init__(self, redis_url: str):\n",
    "        self.redis = Redis.from_url(redis_url)\n",
    "        self.cache_ttl = 3600\n",
    "\n",
    "    async def get_or_set(self, key: str, fetch_func, ttl: Optional[int] = None):\n",
    "        if cached := self.redis.get(key):\n",
    "            return json.loads(cached)\n",
    "        value = await fetch_func()\n",
    "        self.redis.set(key, json.dumps(value), ex=ttl or self.cache_ttl)\n",
    "        return value\n",
    "\n",
    "\n",
    "class APIClient:\n",
    "    def __init__(self, session: aiohttp.ClientSession, config: Settings):\n",
    "        self.session = session\n",
    "        self.config = config\n",
    "        self._rate_limit_delay = 0.1\n",
    "\n",
    "    async def fetch_with_retry(self, url: str, **kwargs):\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                async with self.session.get(url, **kwargs) as response:\n",
    "                    return await response.json()\n",
    "            except Exception as e:\n",
    "                if attempt == 2:\n",
    "                    raise\n",
    "                await asyncio.sleep(2**attempt)\n",
    "\n",
    "\n",
    "class ImageService:\n",
    "    def __init__(self, api_client: APIClient):\n",
    "        self.api_client = api_client\n",
    "        self.base_delay = 10  # Start with a 20-second delay\n",
    "\n",
    "    async def generate_image(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_dir: str,\n",
    "        width: int = 1024,\n",
    "        height: int = 768,\n",
    "        inference_steps: int = 50,\n",
    "        guidance_scale: float = 7.5,\n",
    "        scheduler: str = \"DDIM\",\n",
    "    ) -> Tuple[Optional[Image.Image], Optional[str]]:\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Implement exponential backoff with initial longer delay\n",
    "            await asyncio.sleep(self.base_delay)\n",
    "\n",
    "            async with self.api_client.session.post(\n",
    "                \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_client.config.HF_API_TOKEN}\"\n",
    "                },\n",
    "                json={\n",
    "                    \"inputs\": prompt,\n",
    "                    \"parameters\": {\n",
    "                            \"width\": width,\n",
    "                            \"height\": height,\n",
    "                            \"inference_steps\": inference_steps,\n",
    "                            \"guidance_scale\": guidance_scale,\n",
    "                            \"scheduler\": scheduler,\n",
    "                        },\n",
    "                },\n",
    "            ) as response:\n",
    "                if response.status == 429:\n",
    "                    # Increase base delay for subsequent requests\n",
    "                    self.base_delay *= 1.5\n",
    "                    logging.info(\n",
    "                        f\"Rate limit hit, increasing delay to {self.base_delay} seconds\"\n",
    "                    )\n",
    "                    return None, None\n",
    "\n",
    "                if response.status == 200:\n",
    "                    image_data = await response.read()\n",
    "                    timestamp = int(time.time())\n",
    "                    filename = f\"image_{timestamp}.png\"\n",
    "                    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "                    with open(output_path, \"wb\") as f:\n",
    "                        f.write(image_data)\n",
    "\n",
    "                    image = Image.open(output_path)\n",
    "                    logging.info(\n",
    "                        f\"Successfully generated and saved image to {output_path}\"\n",
    "                    )\n",
    "                    return image, output_path\n",
    "                else:\n",
    "                    logging.error(f\"API request failed with status {response.status}\")\n",
    "                    return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Image generation failed: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, services: Dict, config: Settings):\n",
    "        self.tavily_client = TavilyClient(api_key=config.TAVILY_API_KEY)\n",
    "        self.config = config\n",
    "        self.cache = services.get(\"cache\")\n",
    "        self.logger = services.get(\"logger\")\n",
    "\n",
    "    async def process_data(self, ticker: str) -> Dict:\n",
    "        try:\n",
    "            # Now using the actual ticker (e.g. BTC-USD) not the category (e.g. CRYPTO)\n",
    "            ticker_data = await self._get_ticker_data(\n",
    "                ticker\n",
    "            )  # This will query for BTC-USD\n",
    "            articles = await self._fetch_articles(\n",
    "                ticker\n",
    "            )  # This will search for BTC-USD news\n",
    "            sentiment_data = await self._analyze_sentiment(articles)\n",
    "\n",
    "            return {\n",
    "                \"ticker_data\": ticker_data,\n",
    "                \"articles\": articles,\n",
    "                \"sentiment_data\": sentiment_data,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            self.logger.log_operation(\"data_pipeline_failed\", error=str(e))\n",
    "            raise\n",
    "\n",
    "    async def _get_ticker_data(self, ticker: str) -> Dict:\n",
    "        cache_key = f\"ticker_data_{ticker}\"\n",
    "\n",
    "        def convert_numpy_types(obj):\n",
    "            if isinstance(obj, np.generic):\n",
    "                return obj.item()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy_types(v) for v in obj]\n",
    "            elif isinstance(obj, tuple):\n",
    "                return tuple(convert_numpy_types(v) for v in obj)\n",
    "            return obj\n",
    "\n",
    "        async def fetch():\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "            info = ticker_obj.info\n",
    "            history = ticker_obj.history(period=\"1mo\")\n",
    "            return convert_numpy_types(\n",
    "                {\n",
    "                    \"info\": info,\n",
    "                    \"price\": history[\"Close\"].iloc[-1],\n",
    "                    \"volume\": history[\"Volume\"].iloc[-1],\n",
    "                    \"change\": (history[\"Close\"].iloc[-1] - history[\"Close\"].iloc[0])\n",
    "                    / history[\"Close\"].iloc[0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return await self.cache.get_or_set(cache_key, fetch)\n",
    "\n",
    "    async def _fetch_articles(self, ticker: str) -> List[Dict]:\n",
    "        cache_key = f\"articles_{ticker}\"\n",
    "\n",
    "        async def fetch():\n",
    "            search_results = self.tavily_client.search(\n",
    "                query=f\"Latest {ticker} news and analysis\",\n",
    "                search_depth=\"advanced\",\n",
    "                max_results=10,\n",
    "            )\n",
    "            return [\n",
    "                {\n",
    "                    \"title\": result[\"title\"],\n",
    "                    \"content\": result[\"content\"],\n",
    "                    \"url\": result[\"url\"],\n",
    "                    \"published_date\": result.get(\"published_date\"),\n",
    "                }\n",
    "                for result in search_results[\"results\"]\n",
    "            ]\n",
    "\n",
    "        return await self.cache.get_or_set(cache_key, fetch)\n",
    "\n",
    "    async def _analyze_sentiment(self, articles: List[Dict]) -> Dict:\n",
    "        combined_text = \" \".join([article[\"content\"] for article in articles])\n",
    "        blob = TextBlob(combined_text)\n",
    "\n",
    "        return {\n",
    "            \"polarity\": blob.sentiment.polarity,\n",
    "            \"subjectivity\": blob.sentiment.subjectivity,\n",
    "            \"article_count\": len(articles),\n",
    "        }\n",
    "\n",
    "\n",
    "class EnhancedSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    async def analyze(self, texts: List[str]) -> Dict[str, Any]:\n",
    "        try:\n",
    "            combined_text = \" \".join(texts)\n",
    "            blob = TextBlob(combined_text)\n",
    "            textblob_sentiment = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "            entity_sentiment = self._get_entity_sentiment(texts)\n",
    "            temporal_sentiment = self._get_temporal_sentiment(texts)\n",
    "\n",
    "            return {\n",
    "                \"overall\": {\n",
    "                    \"polarity\": textblob_sentiment,\n",
    "                    \"subjectivity\": subjectivity,\n",
    "                },\n",
    "                \"entities\": entity_sentiment,\n",
    "                \"temporal\": temporal_sentiment,\n",
    "                \"confidence_score\": self._calculate_confidence(\n",
    "                    textblob_sentiment, subjectivity\n",
    "                ),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise\n",
    "\n",
    "    def _get_textblob_sentiment(self, texts: List[str]) -> float:\n",
    "        combined_text = \" \".join(texts)\n",
    "        blob = TextBlob(combined_text)\n",
    "        return blob.sentiment.polarity\n",
    "\n",
    "    def _get_entity_sentiment(self, texts: List[str]) -> Dict[str, float]:\n",
    "        combined_text = \" \".join(texts)\n",
    "        doc = self.nlp(combined_text)\n",
    "        entities = doc.ents\n",
    "        entity_sentiments = {}\n",
    "        for entity in entities:\n",
    "            entity_text = entity.text\n",
    "            blob = TextBlob(entity_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "            entity_sentiments[entity_text] = sentiment\n",
    "        return entity_sentiments\n",
    "\n",
    "    def _get_temporal_sentiment(\n",
    "        self, texts: List[str], num_periods: int = 3\n",
    "    ) -> List[float]:\n",
    "        total_texts = len(texts)\n",
    "        if total_texts == 0:\n",
    "            return []\n",
    "        period_size = max(1, total_texts // num_periods)\n",
    "        sentiments = []\n",
    "        for i in range(num_periods):\n",
    "            start = i * period_size\n",
    "            end = (i + 1) * period_size if i != num_periods - 1 else total_texts\n",
    "            period_texts = texts[start:end]\n",
    "            combined_period_text = \" \".join(period_texts)\n",
    "            blob = TextBlob(combined_period_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "            sentiments.append(sentiment)\n",
    "        return sentiments\n",
    "\n",
    "    def _calculate_confidence(self, polarity: float, subjectivity: float) -> float:\n",
    "        # Confidence calculation: higher subjectivity means lower confidence\n",
    "        # Confidence = 1 - subjectivity\n",
    "        return 1 - subjectivity\n",
    "\n",
    "\n",
    "class PortfolioProcessor:\n",
    "    def __init__(self, services: Dict, logger: StructuredLogger):\n",
    "        self.services = services\n",
    "        self.logger = logger\n",
    "\n",
    "    def read_portfolio(self) -> Dict[str, List[str]]:\n",
    "        try:\n",
    "            with open(\"portfolio.json\", \"r\") as f:\n",
    "                portfolio = json.load(f)\n",
    "                # Log the actual tickers (not categories)\n",
    "                self.logger.log_operation(\n",
    "                    \"portfolio_loaded\",\n",
    "                    tickers_count=sum(len(v) for v in portfolio.values()),\n",
    "                )\n",
    "                return portfolio  # Ensure this returns category-to-ticker mappings\n",
    "        except Exception as e:\n",
    "            self.logger.log_operation(\"portfolio_read_failed\", error=str(e))\n",
    "            return {\"crypto\": [], \"forex\": [], \"stocks\": []}\n",
    "\n",
    "\n",
    "class CompleteBlogGenerator:\n",
    "    def __init__(self):\n",
    "        self.config = Settings()\n",
    "        self.setup_components()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\", api_key=self.config.GROQ_API_KEY\n",
    "        )\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def setup_components(self):\n",
    "        self.logger = StructuredLogger()\n",
    "        self.cache = CacheManager(self.config.REDIS_URL)\n",
    "        self.setup_services()\n",
    "\n",
    "    def setup_services(self):\n",
    "        # Create a single session to be shared across services\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        self.api_client = APIClient(self.session, self.config)\n",
    "\n",
    "        self.services = {\n",
    "            \"logger\": self.logger,\n",
    "            \"cache\": self.cache,\n",
    "            \"api_client\": self.api_client,\n",
    "        }\n",
    "\n",
    "        self.services.update(\n",
    "            {\n",
    "                \"data_pipeline\": DataPipeline(self.services, self.config),\n",
    "                \"sentiment\": EnhancedSentimentAnalyzer(),\n",
    "                \"portfolio\": PortfolioProcessor(self.services, self.logger),\n",
    "                \"image\": ImageService(self.api_client),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    async def save_blog_post(self, category: str, ticker: str, content: Dict):\n",
    "        try:\n",
    "            output_dir = f\"output/{category}/posts\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            filename = f\"{ticker}_{datetime.now().strftime('%Y%m%d%H%M%S')}.md\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(output_path, \"w\") as f:\n",
    "                # Title and timestamp\n",
    "                f.write(f\"# {content['title']}\\n\\n\")\n",
    "                f.write(f\"**Timestamp:** {content['timestamp']}\\n\\n\")\n",
    "\n",
    "                # Display the generated image with a descriptive caption\n",
    "                if content.get(\"image_path\"):\n",
    "                    f.write(\"## Market Analysis Visualization\\n\\n\")\n",
    "                    f.write(\n",
    "                        f\"![Market Analysis for {ticker}]({content['image_path']})\\n\\n\"\n",
    "                    )\n",
    "                    f.write(\n",
    "                        f\"*Generated visualization based on market analysis for {ticker}*\\n\\n\"\n",
    "                    )\n",
    "\n",
    "                # Rest of the content\n",
    "                f.write(f\"**Content:**\\n\\n{content['content']}\\n\\n\")\n",
    "\n",
    "                # Sentiment analysis\n",
    "                f.write(\"**Sentiment Analysis:**\\n\\n\")\n",
    "                sentiment = content.get(\"sentiment\", {})\n",
    "                f.write(f\"- Polarity: {sentiment.get('polarity', 'N/A')}\\n\")\n",
    "                f.write(f\"- Subjectivity: {sentiment.get('subjectivity', 'N/A')}\\n\")\n",
    "                f.write(f\"- Article Count: {sentiment.get('article_count', 'N/A')}\\n\\n\")\n",
    "\n",
    "                f.write(f\"**Image Prompt:**\\n\\n{content.get('image_prompt', '')}\\n\\n\")\n",
    "\n",
    "            logging.info(f\"Saved blog post for {ticker} to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save blog post for {ticker}: {e}\")\n",
    "\n",
    "    def clean_html(self, html_text: str) -> str:\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        text = html.unescape(text)\n",
    "        return text\n",
    "\n",
    "    def extract_nouns(self, text: str) -> List[str]:\n",
    "        # Convert dict to string if needed\n",
    "        if isinstance(text, dict):\n",
    "            text = str(text)\n",
    "        doc = self.nlp(text)\n",
    "        nouns = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "        return list(set(nouns))\n",
    "\n",
    "    def summarize_with_spacy(self, text: str, max_sentences: int = 3) -> str:\n",
    "        # Convert dict to string if needed\n",
    "        if isinstance(text, dict):\n",
    "            text = str(text)\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        word_freq = {}\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                word_freq[token.text] = word_freq.get(token.text, 0) + 1\n",
    "\n",
    "        sentence_scores = {}\n",
    "        for sent in sentences:\n",
    "            score = sum(word_freq.get(word, 0) for word in sent.split())\n",
    "            sentence_scores[sent] = score\n",
    "\n",
    "        summary_sentences = sorted(\n",
    "            sentence_scores.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:max_sentences]\n",
    "\n",
    "        return \" \".join(sent[0] for sent in summary_sentences)\n",
    "\n",
    "    def generate_refined_prompt_for_blog_post(self, post_content: str) -> str:\n",
    "        summary = self.summarize_with_spacy(post_content)\n",
    "        nouns = self.extract_nouns(summary)\n",
    "        key_elements = \", \".join(nouns[:7])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Create a visually-rich prompt for AI image generation based on the summary of this blog post:\n",
    "\n",
    "        {summary}\n",
    "\n",
    "        Key visual themes: {key_elements}\n",
    "\n",
    "        The prompt should:\n",
    "        1. Be concise and focused on visual elements.\n",
    "        2. Seamlessly incorporate key visual concepts without explicitly listing them.\n",
    "        3. Maintain a professional tone inline with {summary}.\n",
    "        4. Be optimized for AI image generation models, emphasizing clarity and detail.\n",
    "\n",
    "        Please provide only the refined prompt without any additional commentary or explanation.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.2-3b-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert at creating prompts for AI image generation.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=100,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "\n",
    "            if response.choices[0].message.content:\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                \"A modern illustration representing key themes from the blog post.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.log_operation(\"prompt_generation_failed\", error=str(e))\n",
    "            return \"A modern illustration representing key themes from the blog post.\"\n",
    "\n",
    "    async def generate_content(self, data: Dict, sentiment_data: Dict) -> Dict:\n",
    "        \"\"\"Generate blog post content using the OpenAI API\"\"\"\n",
    "        try:\n",
    "            # Extract sentiment values directly from sentiment_data\n",
    "            sentiment_value = data[\"sentiment_data\"][\"polarity\"]\n",
    "            confidence_value = data[\"sentiment_data\"][\"subjectivity\"]\n",
    "\n",
    "            # Get ticker data directly\n",
    "            ticker_name = data[\"ticker_data\"][\"info\"][\"shortName\"]\n",
    "            ticker_price = float(data[\"ticker_data\"][\"price\"])\n",
    "            ticker_change = float(data[\"ticker_data\"][\"change\"])\n",
    "\n",
    "            # Get article titles with list comprehension\n",
    "            article_titles = \" \".join(\n",
    "                article[\"title\"] for article in data[\"articles\"][:3]\n",
    "            )\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Write a detailed blog post analyzing {ticker_name} based on:\n",
    "            \n",
    "            Market Data:\n",
    "            - Current Price: ${ticker_price:.2f}\n",
    "            - Price Change: {ticker_change:.2%}\n",
    "            \n",
    "            Sentiment Analysis:\n",
    "            - Overall Sentiment: {sentiment_value:.2f}\n",
    "            - Confidence Score: {confidence_value:.2f}\n",
    "            \n",
    "            Recent News:\n",
    "            {article_titles}\n",
    "            \n",
    "            Include technical analysis, market sentiment discussion, and future outlook.\n",
    "            Format in Markdown.\n",
    "            \"\"\"\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama-3.2-3b-preview\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a professional financial analyst and writer.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"title\": f\"Analysis: {ticker_name}\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sentiment\": data[\"sentiment_data\"],\n",
    "                \"image_prompt\": self.generate_refined_prompt_for_blog_post(\n",
    "                    response.choices[0].message.content\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            self.logger.log_operation(\"content_generation_failed\", error=str(e))\n",
    "            raise\n",
    "\n",
    "    async def process_ticker(self, category: str, ticker: str):\n",
    "        try:\n",
    "            logging.info(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "            # Fetch data specific to the ticker\n",
    "            logging.debug(f\"Fetching data for ticker: {ticker}\")\n",
    "            data = await self.services[\"data_pipeline\"].process_data(ticker)\n",
    "\n",
    "            # Analyze sentiment for related news articles\n",
    "            logging.debug(f\"Analyzing sentiment for ticker: {ticker}\")\n",
    "            sentiment = await self.services[\"sentiment\"].analyze(\n",
    "                [article[\"content\"] for article in data[\"articles\"]]\n",
    "            )\n",
    "\n",
    "            # Generate blog content\n",
    "            logging.debug(f\"Generating content for ticker: {ticker}\")\n",
    "            content = await self.generate_content(data, sentiment)\n",
    "\n",
    "            # Add a delay before each API request to avoid hitting rate limits\n",
    "            await asyncio.sleep(3)  # Adjust the delay as needed\n",
    "\n",
    "            # Generate an image prompt and create an image\n",
    "            logging.debug(f\"Generating image prompt for ticker: {ticker}\")\n",
    "            image_prompt = self.generate_refined_prompt_for_blog_post(content)\n",
    "\n",
    "            # Add a delay before each API request to avoid hitting rate limits\n",
    "            await asyncio.sleep(2)  # Adjust the delay as needed\n",
    "\n",
    "            logging.debug(f\"Generating image for ticker: {ticker}\")\n",
    "            image_result = await self.services[\"image\"].generate_image(\n",
    "                image_prompt, f\"output/{category}/images\"\n",
    "            )\n",
    "\n",
    "            # Add the generated image path to the blog content\n",
    "            content[\"image_path\"] = image_result[1] if image_result[1] else None\n",
    "\n",
    "            # Save the blog post\n",
    "            logging.debug(f\"Saving blog post for ticker: {ticker}\")\n",
    "            await self.save_blog_post(category, ticker, content)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ticker processing failed for {ticker}: {e}\")\n",
    "\n",
    "    async def run(self):\n",
    "        portfolio = self.services[\"portfolio\"].read_portfolio()\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for category, tickers in portfolio.items():\n",
    "                for ticker in tickers:\n",
    "                    tasks.append(self.process_ticker(category, ticker))\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            for result in results:\n",
    "                if isinstance(result, Exception):\n",
    "                    logging.error(f\"Task failed with exception: {result}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = CompleteBlogGenerator()\n",
    "    asyncio.run(generator.run())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
