{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated and saved to ./images\\image_1732715413.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    output_dir=\".\",\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=9,\n",
    "    seed=None,\n",
    "    scheduler=\"heunpp2\",\n",
    "):\n",
    "    API_URL = (\n",
    "        \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "    )\n",
    "    API_TOKEN = os.getenv(\"HF_API_TOKEN\")  # Replace with your actual Hugging Face API token\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    seed = random.randint(0, 2**32 - 1) if seed is None else seed\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"seed\": seed,\n",
    "            \"scheduler\": scheduler,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Remove None values from the payload\n",
    "    payload[\"parameters\"] = {\n",
    "        k: v for k, v in payload[\"parameters\"].items() if v is not None\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"API request failed with status code {response.status_code}: {response.text}\"\n",
    "        )\n",
    "\n",
    "    # Get the image from the response\n",
    "    image_bytes = response.content\n",
    "\n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Generate Unix timestamp\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Create filename with Unix timestamp\n",
    "    filename = f\"image_{timestamp}.png\"\n",
    "\n",
    "    # Combine output directory and filename\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return image, output_path\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"The visual scene unfolds in a dynamic, 3D environment with a cityscape at dusk. The Bitcoin (BTC) logo is prominently displayed in the foreground, positioned on a sleek, metallic pedestal. The pedestal is surrounded by a halo of LED screens displaying real-time market data, with charts and graphs radiating from the center.\"\n",
    "\n",
    "    os.makedirs(\"./images\", exist_ok=True)\n",
    "\n",
    "    output_dir = \"./images\"  # Current directory\n",
    "\n",
    "    try:\n",
    "        generated_image, saved_path = generate_image(prompt, output_dir)\n",
    "        print(f\"Image generated and saved to {saved_path}\")\n",
    "        generated_image.show()  # Display the image\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    output_dir=\".\",\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=9,\n",
    "    scheduler=\"DDIM\",\n",
    "):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "    API_TOKEN = \"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\"  # Replace with your actual Hugging Face API token\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"scheduler\": scheduler,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Remove None values from the payload\n",
    "    payload[\"parameters\"] = {\n",
    "        k: v for k, v in payload[\"parameters\"].items() if v is not None\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    # Get the image from the response\n",
    "    image_bytes = response.content\n",
    "\n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Generate Unix timestamp\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Create filename with Unix timestamp\n",
    "    filename = f\"image_{timestamp}.png\"\n",
    "\n",
    "    # Combine output directory and filename\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return image, output_path\n",
    "\n",
    "\n",
    "def split_blog_posts(blog_text):\n",
    "    \"\"\"Split blog posts based on headers and remove 'Similar Articles' section.\"\"\"\n",
    "    # Regex pattern to match the headers (titles) and content sections\n",
    "    pattern = r\"(?<=\\n|^)(# .+?)(?=\\n# |\\Z)\"  # Matches each `#` section until the next `#`\n",
    "    \n",
    "    matches = re.finditer(r\"^# .+?(?=\\n# |\\Z)\", blog_text, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    for match in matches:\n",
    "        # Split title and content based on the first newline\n",
    "        match_text = match.group()\n",
    "        lines = match_text.strip().split('\\n', 1)\n",
    "        title = lines[0].strip()  # The title is the header line (e.g., '# Title')\n",
    "        content = lines[1].strip() if len(lines) > 1 else ''  # The rest is the content\n",
    "        \n",
    "        # Remove the \"Similar articles\" and \"Original article\" sections if present\n",
    "        content = re.sub(r\"\\n(Similar articles:|Original article:).*\", \"\", content, flags=re.DOTALL)\n",
    "        \n",
    "        posts.append({\"title\": title, \"content\": content})\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def generate_prompt_for_blog_post(post):\n",
    "    \"\"\"Generate a text-to-image prompt based on the blog post content.\"\"\"\n",
    "    if \"cloud\" in post[\"title\"].lower():\n",
    "        return (\n",
    "            f\"A modern, futuristic digital illustration showing a vibrant cloud network with interconnected data streams and AI algorithms. \"\n",
    "            \"The cloud is depicted as a floating, glowing entity with nodes and connections representing the flow of information. \"\n",
    "            \"Real-time network visibility is symbolized by dynamic, pulsing light paths, while abstract AI-powered tools are shown optimizing the cloud's operations. \"\n",
    "            \"A business professional is observing the scene, symbolizing the role of IT agility and cloud cost optimization. \"\n",
    "            \"The atmosphere is high-tech, sleek, and professional, with a mix of blue and purple tones to represent innovation and technology. \"\n",
    "            \"The overall composition conveys the idea of revolutionizing cloud management with cutting-edge AI and network visibility.\"\n",
    "        )\n",
    "    elif \"ai\" in post[\"title\"].lower():\n",
    "        return (\n",
    "            f\"A modern digital illustration showing the powerful integration of AI tools in data management. \"\n",
    "            \"The scene features AI algorithms at work, automating and optimizing data pipelines for a SaaS business. \"\n",
    "            \"Data streams are visualized as vibrant light paths, while AI algorithms are represented by dynamic, abstract visuals processing data. \"\n",
    "            \"The overall atmosphere is sleek and futuristic, with a focus on efficiency, innovation, and the role of AI in modern business operations.\"\n",
    "        )\n",
    "    elif \"robot\" in post[\"title\"].lower():\n",
    "        return (\n",
    "            f\"A futuristic surgical operation room with advanced robotics and AI assisting in minimally invasive surgery. \"\n",
    "            \"The scene shows a robotic arm inside an MRI, guided by AI to perform precise operations like biopsies and ablations. \"\n",
    "            \"The surgeon is overseeing the process, interacting with the robot to ensure precision and safety. \"\n",
    "            \"The background is a sleek, high-tech medical environment with vibrant, cool tones, highlighting the cutting-edge nature of the technology.\"\n",
    "        )\n",
    "    elif \"pitch\" in post[\"title\"].lower():\n",
    "        return (\n",
    "            f\"A professional and sleek digital illustration showing a startup founder pitching to venture capitalists. \"\n",
    "            \"The founder is presenting a pitch deck, while the VCs in the background are actively listening and offering feedback. \"\n",
    "            \"The atmosphere is modern and business-like, with a dynamic mix of colors to represent the high-energy startup world. \"\n",
    "            \"The focus is on the storytelling aspect of the pitch, conveying innovation, strategy, and the power of persuasive presentation.\"\n",
    "        )\n",
    "    else:\n",
    "        return \"A generic tech scene representing innovation and the future of technology.\"\n",
    "\n",
    "\n",
    "def process_blog_directory(directory_path, output_dir):\n",
    "    \"\"\"Process all markdown files in the specified directory.\"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                blog_text = file.read()\n",
    "\n",
    "            # Split the blog into individual posts\n",
    "            posts = split_blog_posts(blog_text)\n",
    "\n",
    "            # Loop through each post, generate prompt, and then generate image\n",
    "            for post in posts:\n",
    "                prompt = generate_prompt_for_blog_post(post)\n",
    "                # Print the prompt\n",
    "                print(f\"Generated prompt for post titled: {post['title']}\\n\")\n",
    "                print(\"Generated Prompt: \", prompt)  # Print the generated prompt\n",
    "\n",
    "                # Generate and save the image\n",
    "                try:\n",
    "                    generated_image, saved_path = generate_image(prompt, output_dir)\n",
    "                    print(f\"Image generated and saved to {saved_path} for post titled: {post['title']}\")\n",
    "                    generated_image.show()  # Display the image\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "                 # Add a delay between requests\n",
    "                time.sleep(2)  # 2-second delay; adjust as needed\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"output/tech/test\"  # Directory containing the .md files\n",
    "    output_dir = \"./images/test\"  # Directory to save generated images\n",
    "\n",
    "    process_blog_directory(directory_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade scipy torch spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load a spaCy model (you can choose a language model such as en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def summarize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Use spaCy to extract named entities and sentences\n",
    "    sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]  # Extract long sentences\n",
    "    summary = \" \".join(sentences[:3])  # Pick top 3 sentences for brevity\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "text = \"\"\"# Reactor could make direct air capture more energy efficient\n",
    "\n",
    "**Revolutionizing Direct Air Capture: A New Era of Energy Efficiency**\n",
    "\n",
    "The quest to combat climate change has led to the development of innovative technologies that can effectively capture carbon dioxide from the atmosphere. One such technology is direct air capture, which involves removing CO2 directly from the air. While it has shown promise, the process has been energy-intensive, making it less viable for widespread adoption.\n",
    "\n",
    "Researchers at Rice University have made a groundbreaking discovery that could change the game. They've developed an electrochemical reactor that significantly reduces the energy consumption required for direct air capture. This modular, three-chambered reactor uses a specially engineered porous solid electrolyte layer to efficiently capture and regenerate carbon dioxide.\n",
    "\n",
    "The reactor's design offers several advantages over existing technologies. It works at room temperature, requires no additional chemicals, and generates no unwanted byproducts. This approach also enables the production of hydrogen, which can be used as a clean energy source.\n",
    "\n",
    "The researchers' findings have the potential to make carbon capture more cost-effective and practical for industries worldwide. With its flexibility and scalability, this technology can be used across various sectors, from chemicals to manufacturing.\n",
    "\n",
    "The implications of this innovation are far-reaching. By reducing energy consumption, direct air capture can become a more viable solution for mitigating climate change. As the world transitions to a low-carbon economy, technologies like this electrochemical reactor will play a crucial role in enabling industries to meet their carbon reduction targets.\n",
    "\n",
    "The future of direct air capture looks brighter than ever, thanks to the innovative work of Rice University researchers. This breakthrough has the potential to revolutionize the way we approach carbon capture, paving the way for a more sustainable future.\"\"\"\n",
    "summary = summarize_with_spacy(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    output_dir=\".\",\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=9,\n",
    "    scheduler=\"heunpp2\",\n",
    "):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "    API_TOKEN = os.getenv('HF_API_TOKENF')  # Replace with your actual Hugging Face API token\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"scheduler\": scheduler,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Remove None values from the payload\n",
    "    payload[\"parameters\"] = {\n",
    "        k: v for k, v in payload[\"parameters\"].items() if v is not None\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"API request failed with status code {response.status_code}: {response.text}\"\n",
    "        )\n",
    "\n",
    "    # Get the image from the response\n",
    "    image_bytes = response.content\n",
    "\n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Generate Unix timestamp\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Create filename with Unix timestamp\n",
    "    filename = f\"image_{timestamp}.png\"\n",
    "\n",
    "    # Combine output directory and filename\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return image, output_path\n",
    "\n",
    "\n",
    "def split_blog_posts(blog_text):\n",
    "    \"\"\"Split blog posts based on headers and remove 'Similar Articles' and 'Original article' sections.\"\"\"\n",
    "    pattern = (\n",
    "        r\"(?<=\\n|^)(# .+?)(?=\\n# |\\Z)\"  # Matches each `#` section until the next `#`\n",
    "    )\n",
    "\n",
    "    matches = re.finditer(r\"^# .+?(?=\\n# |\\Z)\", blog_text, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    for match in matches:\n",
    "        # Split title and content based on the first newline\n",
    "        match_text = match.group()\n",
    "        lines = match_text.strip().split(\"\\n\", 1)\n",
    "        title = lines[0].strip()  # The title is the header line (e.g., '# Title')\n",
    "        content = lines[1].strip() if len(lines) > 1 else \"\"  # The rest is the content\n",
    "\n",
    "        # Remove the \"Similar articles\" and \"Original article\" sections if present\n",
    "        content = re.sub(\n",
    "            r\"\\n(Similar articles:|Original article:).*\", \"\", content, flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        posts.append({\"title\": title, \"content\": content})\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def summarize_with_spacy(text):\n",
    "    \"\"\"Summarize the text using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Select sentences that seem relevant or contain key entities\n",
    "    sentences = [\n",
    "        sent.text for sent in doc.sents if len(sent.text.split()) > 5\n",
    "    ]  # Extract long sentences\n",
    "    summary = \" \".join(sentences[:5])  # Pick top 3 sentences for brevity\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_prompt_for_blog_post(post):\n",
    "    \"\"\"Generate a dynamic text-to-image prompt based on the blog post content.\"\"\"\n",
    "    # Summarize the content of the post using spaCy\n",
    "    summary = summarize_with_spacy(post[\"content\"])\n",
    "\n",
    "    # Build the prompt based on the summary without any predefined themes\n",
    "    return f\"\"\"Create a digital illustration that accurately represents the following blog post summary:\n",
    "\n",
    "        {summary}\n",
    "        \n",
    "        Ensure the illustration:\n",
    "            - Clearly depicts the main topic and key concepts from the blog post.\n",
    "            - Incorporates relevant technological or innovative elements if applicable.\n",
    "            - Is visually appealing and suitable for a professional blog post.\n",
    "            - The style should be modern, professional, tech-oriented.\n",
    "\n",
    "        The final image should be a cohesive representation that a viewer can immediately connect to the blog post's content.\"\"\"\n",
    "\n",
    "def process_blog_directory(directory_path, output_dir):\n",
    "    \"\"\"Process all markdown files in the specified directory.\"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                blog_text = file.read()\n",
    "\n",
    "            # Split the blog into individual posts\n",
    "            posts = split_blog_posts(blog_text)\n",
    "\n",
    "            # Loop through each post, generate prompt, and then generate image\n",
    "            for post in posts:\n",
    "                prompt = generate_prompt_for_blog_post(post)\n",
    "                # Print the prompt\n",
    "                print(f\"Generated prompt for post titled: {post['title']}\\n\")\n",
    "                print(\"Generated Prompt: \", prompt)  # Print the generated prompt\n",
    "\n",
    "                # Generate and save the image\n",
    "                try:\n",
    "                    generated_image, saved_path = generate_image(prompt, output_dir)\n",
    "                    print(\n",
    "                        f\"Image generated and saved to {saved_path} for post titled: {post['title']}\"\n",
    "                    )\n",
    "                    generated_image.show()  # Display the image\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "                # Add a delay between requests\n",
    "                time.sleep(2)  # 2-second delay; adjust as needed\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"output/tech/test\"  # Directory containing the .md files\n",
    "    output_dir = \"./images/test\"  # Directory to save generated images\n",
    "\n",
    "    process_blog_directory(directory_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return list(set(nouns))  # Return unique nouns\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    output_dir=\".\",\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=9,\n",
    "    scheduler=\"heunpp2\",\n",
    "):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "    API_TOKEN = os.getenv(\"HF_API_TOKEN\")  # Replace with your actual Hugging Face API token\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"scheduler\": scheduler,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Remove None values from the payload\n",
    "    payload[\"parameters\"] = {\n",
    "        k: v for k, v in payload[\"parameters\"].items() if v is not None\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"API request failed with status code {response.status_code}: {response.text}\"\n",
    "        )\n",
    "\n",
    "    # Get the image from the response\n",
    "    image_bytes = response.content\n",
    "\n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Generate Unix timestamp\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Create filename with Unix timestamp\n",
    "    filename = f\"image_{timestamp}.png\"\n",
    "\n",
    "    # Combine output directory and filename\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return image, output_path\n",
    "\n",
    "def split_blog_posts(blog_text):\n",
    "    \"\"\"Split blog posts based on headers and remove 'Similar Articles' and 'Original article' sections.\"\"\"\n",
    "    pattern = (\n",
    "        r\"(?<=\\n|^)(# .+?)(?=\\n# |\\Z)\"  # Matches each `#` section until the next `#`\n",
    "    )\n",
    "\n",
    "    matches = re.finditer(r\"^# .+?(?=\\n# |\\Z)\", blog_text, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    for match in matches:\n",
    "        # Split title and content based on the first newline\n",
    "        match_text = match.group()\n",
    "        lines = match_text.strip().split(\"\\n\", 1)\n",
    "        title = lines[0].strip()  # The title is the header line (e.g., '# Title')\n",
    "        content = lines[1].strip() if len(lines) > 1 else \"\"  # The rest is the content\n",
    "\n",
    "        # Remove the \"Similar articles\" and \"Original article\" sections if present\n",
    "        content = re.sub(\n",
    "            r\"\\n(Similar articles:|Original article:).*\", \"\", content, flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        posts.append({\"title\": title, \"content\": content})\n",
    "\n",
    "    return posts\n",
    "\n",
    "def summarize_with_spacy(text):\n",
    "    \"\"\"Summarize the text using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Select sentences that seem relevant or contain key entities\n",
    "    sentences = [\n",
    "        sent.text for sent in doc.sents if len(sent.text.split()) > 5\n",
    "    ]  # Extract long sentences\n",
    "    summary = \" \".join(sentences[:5])  # Pick top 5 sentences for brevity\n",
    "    return summary\n",
    "\n",
    "def generate_prompt_for_blog_post(post):\n",
    "    \"\"\"Generate a dynamic text-to-image prompt based on the blog post content.\"\"\"\n",
    "    # Summarize the content of the post using spaCy\n",
    "    summary = summarize_with_spacy(post[\"content\"])\n",
    "    \n",
    "    # Extract nouns from the summary\n",
    "    nouns = extract_nouns(summary)\n",
    "    key_elements = ', '.join(nouns[:5])  # Use top 5 nouns as key elements\n",
    "\n",
    "    # Build the prompt based on the summary and key elements\n",
    "    return f\"\"\"Create a digital illustration that accurately represents the following blog post summary:\n",
    "\n",
    "        {summary}\n",
    "        \n",
    "        Key elements: {key_elements}\n",
    "\n",
    "        Ensure the illustration:\n",
    "            - Clearly depicts the main topic and key concepts from the blog post.\n",
    "            - Incorporates relevant technological or innovative elements if applicable.\n",
    "            - Is visually appealing and suitable for a professional blog post.\n",
    "            - The style should be modern, professional, tech-oriented.\n",
    "\n",
    "        The final image should be a cohesive representation that a viewer can immediately connect to the blog post's content.\"\"\"\n",
    "\n",
    "def process_blog_directory(directory_path, output_dir):\n",
    "    \"\"\"Process all markdown files in the specified directory.\"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                blog_text = file.read()\n",
    "\n",
    "            # Split the blog into individual posts\n",
    "            posts = split_blog_posts(blog_text)\n",
    "\n",
    "            # Loop through each post, generate prompt, and then generate image\n",
    "            for post in posts:\n",
    "                prompt = generate_prompt_for_blog_post(post)\n",
    "                # Print the prompt\n",
    "                print(f\"Generated prompt for post titled: {post['title']}\\n\")\n",
    "                print(\"Generated Prompt: \", prompt)  # Print the generated prompt\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # Generate and save the image\n",
    "                try:\n",
    "                    generated_image, saved_path = generate_image(prompt, output_dir)\n",
    "                    print(\n",
    "                        f\"Image generated and saved to {saved_path} for post titled: {post['title']}\"\n",
    "                    )\n",
    "                    print(\"\\n\")\n",
    "                    generated_image.show()  # Display the image\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "                # Add a delay between requests\n",
    "                time.sleep(2)  # 2-second delay; adjust as needed\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"output/tech/test\"  # Directory containing the .md files\n",
    "    output_dir = \"./images/test\"  # Directory to save generated images\n",
    "\n",
    "    process_blog_directory(directory_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------\n",
    "## With refined prompt\n",
    "### ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import time\n",
    "import spacy\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return list(set(nouns))  # Return unique nouns\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    prompt,\n",
    "    output_dir=\".\",\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=9,\n",
    "    scheduler=\"heunpp2\",\n",
    "):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-dev\"\n",
    "    API_TOKEN = os.getenv(\n",
    "        \"HF_API_TOKEN\"\n",
    "    )  # Replace with your actual Hugging Face API token\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "            \"guidance_scale\": guidance_scale,\n",
    "            \"scheduler\": scheduler,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Remove None values from the payload\n",
    "    payload[\"parameters\"] = {\n",
    "        k: v for k, v in payload[\"parameters\"].items() if v is not None\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"API request failed with status code {response.status_code}: {response.text}\"\n",
    "        )\n",
    "\n",
    "    # Get the image from the response\n",
    "    image_bytes = response.content\n",
    "\n",
    "    # Convert bytes to PIL Image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # Generate Unix timestamp\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Create filename with Unix timestamp\n",
    "    filename = f\"image_{timestamp}.png\"\n",
    "\n",
    "    # Combine output directory and filename\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return image, output_path\n",
    "\n",
    "\n",
    "def split_blog_posts(blog_text):\n",
    "    \"\"\"Split blog posts based on headers and remove 'Similar Articles' and 'Original article' sections.\"\"\"\n",
    "    pattern = (\n",
    "        r\"(?<=\\n|^)(# .+?)(?=\\n# |\\Z)\"  # Matches each `#` section until the next `#`\n",
    "    )\n",
    "\n",
    "    matches = re.finditer(r\"^# .+?(?=\\n# |\\Z)\", blog_text, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    for match in matches:\n",
    "        # Split title and content based on the first newline\n",
    "        match_text = match.group()\n",
    "        lines = match_text.strip().split(\"\\n\", 1)\n",
    "        title = lines[0].strip()  # The title is the header line (e.g., '# Title')\n",
    "        content = lines[1].strip() if len(lines) > 1 else \"\"  # The rest is the content\n",
    "\n",
    "        # Remove the \"Similar articles\" and \"Original article\" sections if present\n",
    "        content = re.sub(\n",
    "            r\"\\n(Similar articles:|Original article:).*\", \"\", content, flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        posts.append({\"title\": title, \"content\": content})\n",
    "\n",
    "    return posts\n",
    "\n",
    "\n",
    "def summarize_with_spacy(text):\n",
    "    \"\"\"Summarize the text using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Select sentences that seem relevant or contain key entities\n",
    "    sentences = [\n",
    "        sent.text for sent in doc.sents if len(sent.text.split()) > 5\n",
    "    ]  # Extract long sentences\n",
    "    summary = \" \".join(sentences[:5])  # Pick top 5 sentences for brevity\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_refined_prompt_for_blog_post(self, post):\n",
    "    \"\"\"Generate a refined text-to-image prompt based on the blog post content.\"\"\"\n",
    "    # Summarize the content of the post using spaCy\n",
    "    summary = self.summarize_with_spacy(post[\"content\"])\n",
    "\n",
    "    # Extract nouns from the summary\n",
    "    nouns = self.extract_nouns(summary)\n",
    "    key_elements = \", \".join(nouns[:5])  # Use top 5 nouns as key elements\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Create a concise, visually-focused prompt for AI image generation based on this blog post summary:\n",
    "\n",
    "    {summary}\n",
    "\n",
    "    Key elements: {key_elements}\n",
    "\n",
    "    The prompt should:\n",
    "    1. Be concise and focused on visual elements\n",
    "    2. Incorporate key concepts without explicitly listing them\n",
    "    3. Maintain a professional, tech-oriented style\n",
    "    4. Be optimized for AI image generation models\n",
    "\n",
    "    Example of a well-structured prompt:\n",
    "    \"A sleek, modern smartphone displaying an innovative AI application. The device floats in a minimalist tech environment with subtle holographic UI elements. Cool blue tones dominate, with occasional bright accents. Sharp focus on the device, with a soft blur on the futuristic background.\"\n",
    "\n",
    "    Provide only the refined prompt without any additional commentary or explanation:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.2-3b-preview\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert at creating prompts for AI image generation.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating refined prompt: {str(e)}\")\n",
    "        # Fallback to a simple prompt if refinement fails\n",
    "        return f\"A modern, tech-oriented illustration representing {key_elements}.\"\n",
    "\n",
    "\n",
    "def process_blog_directory(directory_path, output_dir):\n",
    "    \"\"\"Process all markdown files in the specified directory.\"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                blog_text = file.read()\n",
    "\n",
    "            # Split the blog into individual posts\n",
    "            posts = split_blog_posts(blog_text)\n",
    "\n",
    "            # Loop through each post, generate prompt, and then generate image\n",
    "            for post in posts:\n",
    "                prompt = generate_prompt_for_blog_post(post)\n",
    "                # Print the prompt\n",
    "                print(f\"Generated prompt for post titled: {post['title']}\\n\")\n",
    "                print(\"Generated Prompt: \", prompt)  # Print the generated prompt\n",
    "                print(\"\\n\")\n",
    "\n",
    "                # Generate and save the image\n",
    "                try:\n",
    "                    generated_image, saved_path = generate_image(prompt, output_dir)\n",
    "                    print(\n",
    "                        f\"Image generated and saved to {saved_path} for post titled: {post['title']}\"\n",
    "                    )\n",
    "                    print(\"\\n\")\n",
    "                    generated_image.show()  # Display the image\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "                # Add a delay between requests\n",
    "                time.sleep(2)  # 2-second delay; adjust as needed\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"output/tech/test\"  # Directory containing the .md files\n",
    "    output_dir = \"./images/test\"  # Directory to save generated images\n",
    "\n",
    "    process_blog_directory(directory_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
